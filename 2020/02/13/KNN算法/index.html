<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Anjhon">


    <meta name="subtitle" content="小安">


    <meta name="description" content="没有做不到的事,只是看你想不想做">



<title>KNN近邻算法 | Anjhon&#39;s Blog</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 4.1.1"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Anjhon&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">博文</a>
                
                    <a class="menu-item" href="/category">分类</a>
                
                    <a class="menu-item" href="/tag">标签</a>
                
                    <a class="menu-item" href="/about">关于</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Anjhon&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">博文</a>
                
                    <a class="menu-item" href="/category">分类</a>
                
                    <a class="menu-item" href="/tag">标签</a>
                
                    <a class="menu-item" href="/about">关于</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">KNN近邻算法</h1>
            
                <div class="post-meta">
                    <br>
                    
                        作者: <a itemprop="author" rel="author" href="/">&nbsp;&nbsp;Anjhon</a>
                    

                    
                        <p class="post-time">
                        发表: <a href="#">&nbsp;&nbsp;2020-02-13</a>
                        </p>
                    
                    
                        <p class="post-category">
                    分类:
                            
                                <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">&nbsp;&nbsp;机器学习</a>
                            
                            </p>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="一-KNN算法概述"><a href="#一-KNN算法概述" class="headerlink" title="一: KNN算法概述"></a>一: KNN算法概述</h1><p><strong>K nearest neighbors (k邻近算法)</strong></p>
<p>KNN是通过测量不同特征值之间的<strong>距离</strong>进行分类。</p>
<p>KNN做分类预测时，一般是选择多数表决法，即训练集里和预测的样本特征最近的K个样本，预测为里面有最多类别数的类别。而KNN做回归时，一般是选择平均法，即最近的K个样本的样本输出的平均值作为回归预测值。</p>
<p>它的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别; 其中K通常是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。</p>
<p>KNN方法既可以做分类，也可以做回归</p>
<p><strong>举例说明</strong></p>
<p><strong>例- 1 : 图片中分三类$$w_1 红色圆圈，w_2 蓝色三角，w_3 绿色的方块$$，请问数据$$X_u$$属于哪个类别</strong></p>
<img src="https://bkimg.cdn.bcebos.com/pic/03087bf40ad162d95867202e15dfa9ec8a13cd73?x-bce-process=image/watermark,g_7,image_d2F0ZXIvYmFpa2U4MA==,xp_5,yp_5" style="zoom:33%;" />

<p>设置k=5, 4个邻居是红色，1个邻居是绿色的; 所以是红色的概率为80%, 是绿色的概率是20%</p>
<p><strong>例 - 2: 如下图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？</strong></p>
<p><img src="https://bkimg.cdn.bcebos.com/pic/bf096b63f6246b60f20ccd5aebf81a4c510fa29a?x-bce-process=image/resize,m_lfit,w_268,limit_1" alt=""></p>
<p>如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。</p>
<p><strong>由此也说明了KNN算法的结果很大程度取决于K的选择。</strong></p>
<br>




<h1 id="二-KNN算法三要素"><a href="#二-KNN算法三要素" class="headerlink" title="二: KNN算法三要素"></a>二: KNN算法三要素</h1><p>KNN算法三要素: <strong>k值的选取</strong>，<strong>距离度量的方式</strong>和<strong>分类决策规则</strong></p>
<h2 id="一-K值得选取"><a href="#一-K值得选取" class="headerlink" title="(一): K值得选取"></a>(一): K值得选取</h2><p>对于k值的选择，没有一个固定的经验，一般根据样本的分布，选择一个较小的值，可以通过交叉验证选择一个合适的k值。</p>
<p>选择较小的k值，就相当于用较小的领域中的训练实例进行预测，训练误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是泛化误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；<br>　　选择较大的k值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少泛化误差，但缺点是训练误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。<br>　　一个极端是k等于样本数m，则完全没有分类，此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单。</p>
<h2 id="二-距离的度量方式-常用欧式距离"><a href="#二-距离的度量方式-常用欧式距离" class="headerlink" title="(二): 距离的度量方式(常用欧式距离)"></a>(二): 距离的度量方式(常用欧式距离)</h2><p><strong>欧式距离:</strong><br>$$<br>d(x, y) =\sqrt{(x_1−y_1)^2+(x_2−y_2)^2+…+(x_n−y_n)^2}=\sqrt{\sum\limits_{i=1}^n(x_i - y_i)^2}<br>$$</p>
<p><strong>曼哈顿距离:</strong><br>$$<br>d(x, y) = |x_1−y_1|+|x_2−y_2|+…+|x_n−y_n|=\sum\limits_{i=1}^n\vert x_i - y_i\vert<br>$$</p>
<p><strong>闵可夫斯基距离:</strong><br>$$<br>d(x,y)= \sqrt[p]{(|x_1−y_1|)^p+(|x_2−y_2|)^p+…+(|x_n−y_n|)^p}=\sqrt[p]{\sum_{i=1}^n(|x_i−y_i|)^p}<br>$$<br>可以看出，欧式距离是闵可夫斯基距离距离在p=2时的特例，而曼哈顿距离是p=1时的特例。</p>
<p><strong>方差和标准差:</strong></p>
<p><img src="https://bkimg.cdn.bcebos.com/pic/472309f79052982281674d51ddca7bcb0b46d4c8?x-bce-process=image/watermark,g_7,image_d2F0ZXIvYmFpa2U5Mg==,xp_5,yp_5" alt=""></p>
<h2 id="三-分类决策规则"><a href="#三-分类决策规则" class="headerlink" title="(三): 分类决策规则"></a>(三): 分类决策规则</h2><p>一般都是使用前面提到的多数表决法。</p>
<br>


<h1 id="三-KNN算法的实现方式"><a href="#三-KNN算法的实现方式" class="headerlink" title="三: KNN算法的实现方式"></a>三: KNN算法的实现方式</h1><h2 id="一-蛮力实现"><a href="#一-蛮力实现" class="headerlink" title="(一): 蛮力实现"></a>(一): 蛮力实现</h2><p>既然我们要找到k个最近的邻居来做预测，那么我们只需要计算预测样本和所有训练集中的样本的距离，然后计算出最小的k个距离即可，接着多数表决，很容易做出预测。这个方法的确简单直接，在样本量少，样本特征少的时候有效。但是在实际运用中很多时候用不上，为什么呢？因为我们经常碰到样本的特征数有上千以上，样本量有几十万以上，如果我们这要去预测少量的测试集样本，算法的时间效率很成问题。因此，这个方法我们一般称之为蛮力实现。比较适合于少量样本的简单模型的时候用。</p>
<h2 id="二-KD树实现原理"><a href="#二-KD树实现原理" class="headerlink" title="(二): KD树实现原理"></a>(二): KD树实现原理</h2><p>KD树算法没有一开始就尝试对测试样本分类，而是先对训练集建模，建立的模型就是KD树，建好了模型再对测试集做预测。所谓的KD树就是K个特征维度的树，注意这里的K和KNN中的K的意思不同。KNN中的K代表最近的K个样本，KD树中的K代表样本特征的维数。为了防止混淆，后面我们称特征维数为n。</p>
<p>KD树算法包括三步，第一步是建树，第二部是搜索最近邻，最后一步是预测。</p>
<h3 id="1-KD树建立"><a href="#1-KD树建立" class="headerlink" title="1: KD树建立"></a>1: KD树建立</h3><p>我们首先来看建树的方法。KD树建树采用的是从m个样本的n维特征中，分别计算n个特征的取值的方差，用方差最大的第k维特征$n_k$来作为根节点。对于这个特征，我们选择特征$n_k$的取值的中位数$n_{kv}$对应的样本作为划分点，对于所有第k维特征的取值小于$n_{kv}$的样本，我们划入左子树，对于第k维特征的取值大于等于$n_{kv}$的样本，我们划入右子树，对于左子树和右子树，我们采用和刚才同样的办法来找方差最大的特征来做更节点，递归的生成KD树。</p>
<p>比如我们有二维样本6个，{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，构建kd树的具体步骤为：</p>
<ul>
<li><p>1）找到划分的特征。6个数据点在x，y维度上的数据方差分别为6.97，5.37，所以在x轴上方差更大，用第1维特征建树。</p>
</li>
<li><p>2）确定划分点（7,2）。根据x维上的值将数据排序，6个数据的中值(所谓中值，即中间大小的值)为7，所以划分点的数据是（7,2）。这样，该节点的分割超平面就是通过（7,2）并垂直于：划分点维度的直线x=7；</p>
</li>
<li><p>3）确定左子空间和右子空间。 分割超平面x=7将整个空间分为两部分：x&lt;=7的部分为左子空间，包含3个节点={(2,3),(5,4),(4,7)}；另一部分为右子空间，包含2个节点={(9,6)，(8,1)}。</p>
</li>
<li><p>4）用同样的办法划分左子树的节点{(2,3),(5,4),(4,7)}和右子树的节点{(9,6)，(8,1)}。最终得到KD树。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201611/1042406-20161114151317201-1936126361.jpg" alt=""></p>
</li>
</ul>
<h3 id="2-KD树搜索最近邻"><a href="#2-KD树搜索最近邻" class="headerlink" title="2: KD树搜索最近邻"></a>2: KD树搜索最近邻</h3><p>当我们生成KD树以后，就可以去预测测试集里面的样本目标点了。对于一个目标点，我们首先在KD树里面找到包含目标点的叶子节点。以目标点为圆心，以目标点到叶子节点样本实例的距离为半径，得到一个超球体，最近邻的点一定在这个超球体内部。然后返回叶子节点的父节点，检查另一个子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点寻找是否有更加近的近邻,有的话就更新最近邻。如果不相交那就简单了，我们直接返回父节点的父节点，在另一个子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。</p>
<p>从上面的描述可以看出，KD树划分后可以大大减少无效的最近邻搜索，很多样本点由于所在的超矩形体和超球体不相交，根本不需要计算距离。大大节省了计算时间。</p>
<p>先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径&lt;(7,2)，(5,4)，(4,7)&gt;，但 （4,7）与目标查找点的距离为3.202，而（5,4）与查找点之间的距离为3.041，所以（5,4）为查询点的最近点； 以（2，4.5）为圆心，以3.041为半径作圆，如下图所示。可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间进行查找，也就是将（2,3）节点加入搜索路径中得&lt;(7,2)，(2,3)&gt;；于是接着搜索至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5；回溯查找至（5,4），直到最后回溯到根结点（7,2）的时候，以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如下图所示。至此，搜索路径回溯完，返回最近邻点（2,3），最近距离1.5。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201611/1042406-20161114165145763-428195796.jpg" alt=""></p>
<h3 id="3-KD树预测"><a href="#3-KD树预测" class="headerlink" title="3: KD树预测"></a>3: KD树预测</h3><p>有了KD树搜索最近邻的办法，KD树的预测就很简单了，在KD树搜索最近邻的基础上，我们选择到了第一个最近邻样本，就把它置为已选。在第二轮中，我们忽略置为已选的样本，重新选择最近邻，这样跑k次，就得到了目标的K个最近邻，然后根据多数表决法，如果是KNN分类，预测为K个最近邻里面有最多类别数的类别。如果是KNN回归，用K个最近邻样本输出的平均值作为回归预测值。</p>
<h2 id="三-球树实现原理"><a href="#三-球树实现原理" class="headerlink" title="(三): 球树实现原理"></a>(三): 球树实现原理</h2><p><strong>KD树和球树类似，主要区别在于球树得到的是节点样本组成的最小超球体，而KD得到的是节点样本组成的超矩形体，这个超球体要与对应的KD树的超矩形体小，这样在做最近邻搜索的时候，可以避免一些无谓的搜索。</strong></p>
<h3 id="1-球树建立"><a href="#1-球树建立" class="headerlink" title="1: 球树建立"></a>1: 球树建立</h3><p>1)  先构建一个超球体，这个超球体是可以包含所有样本的最小球体。</p>
<p>2) 从球中选择一个离球的中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个上，然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径。这样我们得到了两个子超球体，和KD树里面的左右子树对应。</p>
<p>3) 对于这两个子超球体，递归执行步骤2). 最终得到了一个球树。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201611/1042406-20161114172004185-213758204.jpg" alt=""></p>
<h3 id="2-球树搜索最近邻"><a href="#2-球树搜索最近邻" class="headerlink" title="2: 球树搜索最近邻"></a>2: 球树搜索最近邻</h3><p>使用球树找出给定目标点的最近邻方法是首先自上而下贯穿整棵树找出包含目标点所在的叶子，并在这个球里找出与目标点最邻近的点，这将<strong>确定出目标点距离它的最近邻点的一个上限值</strong>，然后跟KD树查找一样，检查兄弟结点，如果<font color=red>目标点到兄弟结点中心的距离超过兄弟结点的半径与当前的上限值之和</font>，那么兄弟结点里<strong>不可能</strong>存在一个更近的点；<font color=red>如果上诉描述相反,说明有可能存在,也有可能不存在，</font><strong>必须进一步检查位于兄弟结点以下的子树。</strong></p>
<p>检查完兄弟节点后，我们向父节点回溯，继续搜索最小邻近值。当回溯到根节点时，此时的最小邻近值就是最终的搜索结果。</p>
<p>从上面的描述可以看出，<strong>KD树在搜索路径优化时使用的是两点之间的距离来判断，而球树使用的是两边之和大于第三边来判断，相对来说球树的判断更加复杂，但是却避免了更多的搜索，这是一个权衡。</strong></p>
<br>


<h1 id="四-KNN算法的思想总结"><a href="#四-KNN算法的思想总结" class="headerlink" title="四: KNN算法的思想总结"></a>四: KNN算法的思想总结</h1><p>就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：</p>
<ul>
<li><p>计算测试数据与各个训练数据之间的距离；</p>
</li>
<li><p>按照距离的递增关系进行排序；</p>
</li>
<li><p>选取距离最小的K个点；</p>
</li>
<li><p>确定前K个点所在类别的出现频率；</p>
</li>
<li><p>返回前K个点中出现频率最高的类别作为测试数据的预测分类。</p>
</li>
</ul>
<br>


<h1 id="五-KNN算法应用"><a href="#五-KNN算法应用" class="headerlink" title="五: KNN算法应用"></a>五: KNN算法应用</h1><h2 id="一-分类应用"><a href="#一-分类应用" class="headerlink" title="(一): 分类应用"></a>(一): 分类应用</h2><h3 id="1-KNN算法实现鸢尾花分类"><a href="#1-KNN算法实现鸢尾花分类" class="headerlink" title="1: KNN算法实现鸢尾花分类"></a>1: KNN算法实现鸢尾花分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score   <span class="comment"># 准确率计算</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X, y = datasets.load_iris(<span class="literal">True</span>)   <span class="comment"># True表示只获取数据和目标值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认划分比例是 4:1</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">1024</span>)   <span class="comment"># random_state=1024设置固定的随机</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)   <span class="comment"># n_neighbors=5设置k值</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line">y_ = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'预测值:\n'</span>, y_)</span><br><span class="line">print(<span class="string">'实际值:\n'</span>, y_test)</span><br><span class="line">print(<span class="string">'准确率1:\n'</span>, accuracy_score(y_test, y_))</span><br><span class="line">print(<span class="string">'准确率2:\n'</span>, knn.score(X_test, y_test))</span><br><span class="line">print(<span class="string">'人工准确率:\n'</span>, (y_ == y_test).mean())</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">预测值:</span></span><br><span class="line"><span class="string"> [1 0 2 2 0 0 1 2 1 0 0 0 1 2 2 0 1 0 2 0 2 0 1 0 2 1 2 2 2 2 1 2 2 2 1 2 1</span></span><br><span class="line"><span class="string"> 1]</span></span><br><span class="line"><span class="string">实际值:</span></span><br><span class="line"><span class="string"> [1 0 2 2 0 0 1 2 1 0 0 0 1 2 1 0 1 0 2 0 2 0 1 0 2 1 2 2 2 2 1 2 2 2 1 1 1</span></span><br><span class="line"><span class="string"> 1]</span></span><br><span class="line"><span class="string">准确率1:</span></span><br><span class="line"><span class="string"> 0.9473684210526315</span></span><br><span class="line"><span class="string">准确率2:</span></span><br><span class="line"><span class="string"> 0.9473684210526315</span></span><br><span class="line"><span class="string">人工准确率:</span></span><br><span class="line"><span class="string"> 0.9473684210526315</span></span><br><span class="line"><span class="string"> '''</span></span><br></pre></td></tr></table></figure>





<h3 id="2-KNN算法诊断癌症"><a href="#2-KNN算法诊断癌症" class="headerlink" title="2: KNN算法诊断癌症"></a>2: KNN算法诊断癌症</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">'./cancer.csv'</span>, sep=<span class="string">'\t'</span>)   <span class="comment"># sep='\t'分隔符是table键</span></span><br><span class="line">y = data[<span class="string">'Diagnosis'</span>]</span><br><span class="line">X = data.iloc[:, <span class="number">2</span>:]</span><br><span class="line">display(y.head(), X.head())   <span class="comment"># 细胞的各项指标</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># for循环寻找最准确的k值</span></span><br><span class="line">score = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> np.arange(<span class="number">2</span>, <span class="number">16</span>):</span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors=k, weights=<span class="string">'uniform'</span>, algorithm=<span class="string">'kd_tree'</span>)</span><br><span class="line">    <span class="comment"># n_neighbors选取的邻居的数量, 默认为5(一般小于总样本呢量的开平放), 例如 样本数为100, 邻居数量小于10</span></span><br><span class="line">    <span class="comment"># weights='distance'各样本权重的不同(最后计算平均值); weights='uniform'各样本的权重统一(默认)(最后计算平均值)</span></span><br><span class="line">    <span class="comment"># algorithm='kd_tree' 寻找邻居的方式为KD树, algorithm=brute(蛮力-计算所有样本到目标值的距离)</span></span><br><span class="line">    knn.fit(X_train, y_train)</span><br><span class="line">    s = knn.score(X_test, y_test)</span><br><span class="line">    score.append([k,s])</span><br><span class="line">score = pd.DataFrame(score, columns=[<span class="string">'k'</span>, <span class="string">'accuracy'</span>])</span><br><span class="line">print(score)</span><br><span class="line">acc = score[<span class="string">'accuracy'</span>]</span><br><span class="line">index = acc.idxmax()   <span class="comment"># 获取accuracy最大时对应的k值</span></span><br><span class="line">print(<span class="string">'最合适的k数量:'</span>, score[<span class="string">'k'</span>][index])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">     k  accuracy</span></span><br><span class="line"><span class="string">0    2  0.923077</span></span><br><span class="line"><span class="string">1    3  0.916084</span></span><br><span class="line"><span class="string">2    4  0.923077</span></span><br><span class="line"><span class="string">3    5  0.916084</span></span><br><span class="line"><span class="string">4    6  0.937063</span></span><br><span class="line"><span class="string">5    7  0.916084</span></span><br><span class="line"><span class="string">6    8  0.937063</span></span><br><span class="line"><span class="string">7    9  0.937063</span></span><br><span class="line"><span class="string">8   10  0.937063</span></span><br><span class="line"><span class="string">9   11  0.937063</span></span><br><span class="line"><span class="string">10  12  0.937063</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">11  13  0.944056</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">12  14  0.930070</span></span><br><span class="line"><span class="string">13  15  0.937063</span></span><br><span class="line"><span class="string">最合适的邻居数量: 13</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">当k=<span class="number">13</span>时, 预测诊断的效果最好;</span><br></pre></td></tr></table></figure>



<h2 id="二-回归应用"><a href="#二-回归应用" class="headerlink" title="(二): 回归应用"></a>(二): 回归应用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor <span class="comment"># 回归</span></span><br><span class="line"><span class="comment"># 魔法指令，预加载,显示画图(新版本不需要)</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X = np.linspace(<span class="number">0</span>,<span class="number">2</span>*np.pi, <span class="number">60</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)   <span class="comment"># 元数据只是1维的,算法使用的数据的结构必须是二维的</span></span><br><span class="line">X_test = np.linspace(<span class="number">0</span>,<span class="number">2</span>*np.pi, <span class="number">256</span>).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">y = np.sin(X)</span><br><span class="line">plt.scatter(X,y)</span><br><span class="line"></span><br><span class="line">knn = KNeighborsRegressor(weights=<span class="string">'distance'</span>)   </span><br><span class="line"><span class="comment"># distance每个样本的权重不一样, 计算带权重的平均值</span></span><br><span class="line"><span class="comment"># uniform每个样本的权重一样, 计算平均值</span></span><br><span class="line">knn.fit(X, y)</span><br><span class="line">y_ = knn.predict(X_test)</span><br><span class="line">plt.plot(X_test, y_, color=<span class="string">'red'</span>)</span><br><span class="line">plt.scatter(X,y, color=<span class="string">'green'</span>)</span><br></pre></td></tr></table></figure>





<h2 id="三-交叉表"><a href="#三-交叉表" class="headerlink" title="(三): 交叉表"></a>(三): 交叉表</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.crosstab(index = y_test, columns=y_, rownames=[<span class="string">'确诊'</span>], colnames=[<span class="string">'预测'</span>],margins=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://images.cnblogs.com/cnblogs_com/anjhon/1913094/o_210107150155pandas%E4%BA%A4%E5%8F%89%E8%A1%A8.png" alt="交叉表详解"></p>
<br>


<h1 id="六-数据预处理"><a href="#六-数据预处理" class="headerlink" title="六: 数据预处理"></a>六: 数据预处理</h1><h2 id="一-数据归一化"><a href="#一-数据归一化" class="headerlink" title="(一): 数据归一化"></a>(一): 数据归一化</h2><h3 id="1-最大值最小值归一化"><a href="#1-最大值最小值归一化" class="headerlink" title="1: 最大值最小值归一化"></a>1: 最大值最小值归一化</h3><p>也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0 - 1]之间。</p>
<p><strong>转换函数如下：</strong><br>$$<br>x_{normalization} = \frac{x - min}{max-min}<br>$$<br><strong>优化上面癌症诊断的例子:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一: 手写最大值最小值归一化</span></span><br><span class="line">X1 = (X - X.min())/(X.max()-X.min())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二: 直接调用最大值最小值归一化</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler   <span class="comment"># 最小值最大值归一化方法</span></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">X1 = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练并诊断(1000次取平均)</span></span><br><span class="line">score = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X1, y)</span><br><span class="line">    knn = KNeighborsClassifier()</span><br><span class="line">    knn.fit(X_train, y_train)</span><br><span class="line">    score += knn.score(X_test, y_test)/<span class="number">1000</span></span><br><span class="line">print(score)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">0.9674265734265755</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">可见,诊断准确率明显提高</span><br></pre></td></tr></table></figure>





<h3 id="2-Z-score归一化"><a href="#2-Z-score归一化" class="headerlink" title="2: Z-score归一化"></a>2: Z-score归一化</h3><p>这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。经过处理的数据符合标准正态分布，即均值为0，标准差为1</p>
<p>转化函数为：<br>$$<br>x_{normalization} = \frac{x - \mu}{\sigma}<br>$$<br>$\mu$是平均值<br>        $\sigma$是标准差</p>
<p><strong>优化上面癌症诊断的例子:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一: 手写Z-score归一化</span></span><br><span class="line">X2 = (X - X.mean())/(X.std())   <span class="comment"># std是标准差,求每个属性的平均值</span></span><br><span class="line">X2.std()   <span class="comment"># Z-score归一化返回的结果的标准差是1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二: 调用Z-score归一化</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X2 = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">score = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X2, y)</span><br><span class="line">    knn = KNeighborsClassifier()</span><br><span class="line">    knn.fit(X_train, y_train)</span><br><span class="line">    score += knn.score(X_test, y_test)/<span class="number">1000</span></span><br><span class="line">print(score)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">0.9649860139860167</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>





<h3 id="3-sigmoid归一化"><a href="#3-sigmoid归一化" class="headerlink" title="3: sigmoid归一化"></a>3: sigmoid归一化</h3><p>转换函数:<br>$$<br>x_{normalization} = \frac{1}{1+e^{-x}}<br>$$<br>代码略</p>
<p>sigmoid函数对本组数据优化效果不好,说明并不适用于本组数据</p>
<h2 id="二-3-sigma剔除异常值"><a href="#二-3-sigma剔除异常值" class="headerlink" title="(二): 3 sigma剔除异常值"></a>(二): 3 sigma剔除异常值</h2><p>原理:<br>$$<br>|𝑋−𝜇|&gt;3𝜎<br>$$</p>
<p>$\mu$是平均值</p>
<p>$\sigma是标准差$</p>
<p>满足以上判断条件的值为异常值</p>
<p><strong>代码实现(剔除异常癌症数据):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier   <span class="comment"># knn</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, MinMaxScaler   <span class="comment"># 归一化方法</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">'./cancer.csv'</span>, sep=<span class="string">'\t'</span>)</span><br><span class="line">X = data.iloc[:, <span class="number">2</span>:]</span><br><span class="line">y = data.iloc[:, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 异常值处理</span></span><br><span class="line"><span class="comment"># 正常值</span></span><br><span class="line">cond = np.abs(X-X.mean(axis=<span class="number">0</span>)) &lt; <span class="number">4</span>*X.std(axis=<span class="number">0</span>)   <span class="comment"># 根据数据调整sigema的系数</span></span><br><span class="line">cond = cond.all(axis=<span class="number">1</span>)  </span><br><span class="line">X1 = X[cond]</span><br><span class="line">y1 = y[cond]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 异常值</span></span><br><span class="line">cond = np.abs(X-X.mean(axis=<span class="number">0</span>)) &gt; <span class="number">4</span>*X.std(axis=<span class="number">0</span>)</span><br><span class="line">cond = cond.any(axis=<span class="number">1</span>)  </span><br><span class="line">X2 = X[cond]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">score = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X1, y1)</span><br><span class="line">    knn = KNeighborsClassifier()</span><br><span class="line">    knn.fit(X_train, y_train)</span><br><span class="line">    score += knn.score(X_test, y_test)/<span class="number">1000</span></span><br><span class="line">print(<span class="string">'数据清洗后的准确率:'</span>, score)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据清洗后的准确率: 0.964276119402978</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>



<p><a href="https://www.cnblogs.com/pinard/p/6061661.html" target="_blank" rel="noopener">借鉴文章 - K近邻法(KNN)原理小结 - 刘建平</a></p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>文章作者:</span>
                        <span>Anjhon</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>文章地址:</span>
                        <span><a href="https://anjhon1994.github.io/2020/02/13/KNN%E7%AE%97%E6%B3%95/">KNN近邻算法</a></span>
                    </p>
                
                
                
                     <p class="copyright-item">
                         <span>灵魂拷问:</span>
                         <span>Do you believe in <strong>DESTINY<strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">✓ 机器学习</a>
                    
                        <a href="/tags/%E5%9F%BA%E7%A1%80/">✓ 基础</a>
                    
                        <a href="/tags/KNN/">✓ KNN</a>
                    
                        <a href="/tags/%E7%AE%97%E6%B3%95/">✓ 算法</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/03/06/sklearn%20-%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92(%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D)/">sklearn - 线性回归(正规方程与梯度下降)</a>
            
            
            <a class="next" rel="next" href="/2020/01/12/Django%E8%BF%9B%E9%98%B6%E7%94%A8%E6%B3%95/">Django进阶</a>
            
        </section>


    </article>
</div>

			
        </div>
		
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© 
		Anjhon | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a>
		
		
		
		<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


		
		
		</span>
    </div>
</footer>

    </div>
	
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/ Relative)","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"left","width":180,"height":350},"mobile":{"show":false},"react":{"opacityDefault":0.7}});</script></body>
</html>
