<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Anjhon">


    <meta name="subtitle" content="小安">


    <meta name="description" content="没有做不到的事,只是看你想不想做">



<title>sklearn - 线性回归(正规方程与梯度下降) | Anjhon&#39;s Blog</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 4.1.1"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Anjhon&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">博文</a>
                
                    <a class="menu-item" href="/category">分类</a>
                
                    <a class="menu-item" href="/tag">标签</a>
                
                    <a class="menu-item" href="/about">关于</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Anjhon&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">博文</a>
                
                    <a class="menu-item" href="/category">分类</a>
                
                    <a class="menu-item" href="/tag">标签</a>
                
                    <a class="menu-item" href="/about">关于</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">sklearn - 线性回归(正规方程与梯度下降)</h1>
            
                <div class="post-meta">
                    <br>
                    
                        作者: <a itemprop="author" rel="author" href="/">&nbsp;&nbsp;Anjhon</a>
                    

                    
                        <p class="post-time">
                        发表: <a href="#">&nbsp;&nbsp;2020-03-06</a>
                        </p>
                    
                    
                        <p class="post-category">
                    分类:
                            
                                <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">&nbsp;&nbsp;机器学习</a>
                            
                            </p>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="一-线性回归方程"><a href="#一-线性回归方程" class="headerlink" title="一: 线性回归方程"></a>一: 线性回归方程</h1><p>​        <strong>线性回归</strong>（英语：linear regression）是利用称为线性回归方程的<strong>最小二乘函数</strong>对一个或多个自变量和因变量</p>
<p>之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量</p>
<p>的情况称为简单回归，大于一个自变量情况的叫做多元回归</p>
<p>​        在线性回归中，数据使用线性预测函数来建模，并且未知的模型参数也是通过数据来估计。这些模型被叫做</p>
<p>线性模型。最常用的线性回归建模是给定X值的y的条件均值是X的仿射函数。不太一般的情况，线性回归模型可以</p>
<p>是一个中位数或一些其他的给定X的条件下y的条件分布的分位数作为X的线性函数表示。像所有形式的回归分析一</p>
<p>样，线性回归也把焦点放在给定X值的y的条件概率分布，而不是X和y的联合概率分布（多元分析领域）。</p>
<p>​        线性回归有很多实际用途。分为以下两大类：</p>
<ol>
<li><p>如果目标是预测或者映射，线性回归可以用来对观测数据集的和X的值拟合出一个预测模型。当完成这样一个</p>
<p>模型以后，对于一个新增的X值，在没有给定与它相配对的y的情况下，可以用这个拟合过的模型预测出一个y</p>
<p>值。</p>
</li>
<li><p>给定一个变量y和一些变量${\displaystyle X_{1}}X_1,…,{\displaystyle X_{p}}X_p$，这些变量有可能与y相关，线性回归分析可以用来量化y与Xj之</p>
<p>间相关性的强度，评估出与y不相关的，${\displaystyle X_{j}}X_j$并识别出哪些${\displaystyle X_{j}}X_j$的子集包含了关于y的冗余信息。</p>
</li>
</ol>
<h2 id="使用sklearn线性回归模型-jupyter"><a href="#使用sklearn线性回归模型-jupyter" class="headerlink" title="使用sklearn线性回归模型(jupyter)"></a>使用sklearn线性回归模型(jupyter)</h2><p><strong>这里我们以波士顿的房价数据来进行使用分析</strong></p>
<h3 id="一-导入sklearn"><a href="#一-导入sklearn" class="headerlink" title="(一): 导入sklearn"></a>(一): 导入sklearn</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性回归,拟合方程,求解系数, 一次幂</span></span><br><span class="line"><span class="comment"># 线性方程:直来直去,不拐弯</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="comment"># 导入数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="comment"># 导入数据分离的方法(获取数据后,一部分数据用来让回归模型学习,另一部分用来预测)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>
<h3 id="二-获取波士顿房价数据"><a href="#二-获取波士顿房价数据" class="headerlink" title="(二): 获取波士顿房价数据"></a>(二): 获取波士顿房价数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取的数据是numpy,ndarray类型</span></span><br><span class="line">data = datasets.load_boston()   </span><br><span class="line"><span class="comment"># 该数据内有完整的影响房价的因素和完整的房价信息,本次实验就是将数据分为两部分, 一部分用来训练模型,另一部分用来预测,最后将预测出来的数据和已有的完整信息进行对比,判断该模型是否适用于这组房价数据</span></span><br><span class="line"></span><br><span class="line">data   <span class="comment"># 查看data的数据结构</span></span><br><span class="line">data.feature_names   <span class="comment"># 查看影响房价的属性名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x是属性,特征,未知数</span></span><br><span class="line">X = data[<span class="string">'data'</span>]</span><br><span class="line">X.shape   <span class="comment"># 运行结果是(506, 13), 506表示样本是506个, 每个样本采集了13个属性特征;13个属性,需要构建构建了13元一次方程</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># y是房价的估值</span></span><br><span class="line">y = data[<span class="string">'target'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># !!!!!!!!!!</span></span><br><span class="line"><span class="comment"># X, y = datasets.load_boston(True) 获取到X, y的值和以上的一样</span></span><br></pre></td></tr></table></figure>
<h3 id="三-使用模型进行预测"><a href="#三-使用模型进行预测" class="headerlink" title="(三): 使用模型进行预测"></a>(三): 使用模型进行预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y)   <span class="comment"># 将数据进行分离(默认是3:1); train_test_split(X, y)函数会随机打乱顺序</span></span><br><span class="line"></span><br><span class="line">display(X_train.shape, X_test.shape)   <span class="comment"># (379, 13) ; (127, 13)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明算法</span></span><br><span class="line">linear = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">linear.fit(X_train, y_train)   <span class="comment"># X_train, y_train是之前分离出来用来训练模型的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_ = linear.predict(X_test).round(<span class="number">1</span>)   <span class="comment"># X_test是影响房价的因素,该预测模型能根据影响房价的因素预测剩余部分的房价</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预估数据和实际数据比较</span></span><br><span class="line">print(y_)</span><br><span class="line">print(y_test)</span><br></pre></td></tr></table></figure>
<p><strong><font color=red>经过估计数据和实际数据对比,说明算法模型适用于数据</font></strong></p>
<h3 id="四-自建方程预测数据-与-使用线性模型得到的数据对比"><a href="#四-自建方程预测数据-与-使用线性模型得到的数据对比" class="headerlink" title="(四): 自建方程预测数据 与 使用线性模型得到的数据对比"></a>(四): 自建方程预测数据 与 使用线性模型得到的数据对比</h3><p>​        假设波士顿的房价数据符合线性回归的特性,则我们可以通过构建线性方程来预测波士顿剩余部分的房价信息</p>
<p>根据一次线性回归方程: $f(X) = Xw+b$ 可推导得出: $ f(X) = w_1x_1+W_2x_2+…+w_{13}x_{13} +b$   (有13个影响房</p>
<p>价的因素)</p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过训练模型,可从模型中得出系数w</span></span><br><span class="line">w_ = linear.coef_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过训练模型,可从模型中得出截距b</span></span><br><span class="line">b_ = linear.intercept_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自建方程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(w_, b_, X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.dot(X, w_)+b_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用方程得到预估的房价信息</span></span><br><span class="line">fun(w_, b_, X_test).round(<span class="number">1</span>)   <span class="comment"># round(1)保留一位小数</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([31.3, 13.4, 28.6, 20.5, 20.4, 19.4, 32.2, 24. , 25.8, 29.5, 24.5,</span></span><br><span class="line"><span class="string">       25.2, 31.9,  8.2, 20.9, 29.3, 22.3, 35.2, 16.4, 18.5, 30.8, 41.1,</span></span><br><span class="line"><span class="string">       16.2, 13.7, 17.7, 23.8,  7.8, 12. , 20.5, 15.3, 29.3, 26.8, 31.8,</span></span><br><span class="line"><span class="string">       26. , 30.4, 39.2, 25.3, 40.7, 11.6, 27.3, 16.7, 18.8, 19.5, 19.9,</span></span><br><span class="line"><span class="string">       20.7, 22.8, 17.4, 21.6, 23.3, 30. , 25.2, 23.7, 34.2, 18.2, 33.5,</span></span><br><span class="line"><span class="string">       16. , 28.3, 14.1, 24.2, 16.2, 16.7, 23.5, 16. , 21.4, 21.8, 28.2,</span></span><br><span class="line"><span class="string">       25.7, 31.2, 18.8, 26.4, 28.3, 21.9, 27.5, 27.1, 27.1, 15. , 26. ,</span></span><br><span class="line"><span class="string">       26.3, 13.2, 13.3, 26.1, 20.5, 16.8, 24.3, 36.6, 21.4,  8.3, 27.8,</span></span><br><span class="line"><span class="string">        3.6, 19.2, 27.5, 33.6, 28.4, 34.3, 28.2, 13.3, 18. , 23.5, 30.4,</span></span><br><span class="line"><span class="string">       32.9, 23.7, 30.5, 19.8, 19.5, 18.7, 30.9, 36.3,  8. , 18.2, 13.9,</span></span><br><span class="line"><span class="string">       15. , 26.4, 24. , 30.2, 20. ,  5.6, 21.4, 22.9, 17.6, 32.8, 22.1,</span></span><br><span class="line"><span class="string">       32.6, 20.9, 19.3, 23.1, 21. , 21.5])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklesrn中的线性模型得到的预估房价信息</span></span><br><span class="line">linear.predict(X_test).round(<span class="number">1</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([31.3, 13.4, 28.6, 20.5, 20.4, 19.4, 32.2, 24. , 25.8, 29.5, 24.5,</span></span><br><span class="line"><span class="string">       25.2, 31.9,  8.2, 20.9, 29.3, 22.3, 35.2, 16.4, 18.5, 30.8, 41.1,</span></span><br><span class="line"><span class="string">       16.2, 13.7, 17.7, 23.8,  7.8, 12. , 20.5, 15.3, 29.3, 26.8, 31.8,</span></span><br><span class="line"><span class="string">       26. , 30.4, 39.2, 25.3, 40.7, 11.6, 27.3, 16.7, 18.8, 19.5, 19.9,</span></span><br><span class="line"><span class="string">       20.7, 22.8, 17.4, 21.6, 23.3, 30. , 25.2, 23.7, 34.2, 18.2, 33.5,</span></span><br><span class="line"><span class="string">       16. , 28.3, 14.1, 24.2, 16.2, 16.7, 23.5, 16. , 21.4, 21.8, 28.2,</span></span><br><span class="line"><span class="string">       25.7, 31.2, 18.8, 26.4, 28.3, 21.9, 27.5, 27.1, 27.1, 15. , 26. ,</span></span><br><span class="line"><span class="string">       26.3, 13.2, 13.3, 26.1, 20.5, 16.8, 24.3, 36.6, 21.4,  8.3, 27.8,</span></span><br><span class="line"><span class="string">        3.6, 19.2, 27.5, 33.6, 28.4, 34.3, 28.2, 13.3, 18. , 23.5, 30.4,</span></span><br><span class="line"><span class="string">       32.9, 23.7, 30.5, 19.8, 19.5, 18.7, 30.9, 36.3,  8. , 18.2, 13.9,</span></span><br><span class="line"><span class="string">       15. , 26.4, 24. , 30.2, 20. ,  5.6, 21.4, 22.9, 17.6, 32.8, 22.1,</span></span><br><span class="line"><span class="string">       32.6, 20.9, 19.3, 23.1, 21. , 21.5])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<p><strong><font color=red>通过自建模型获取预估数据与使用模型获取预估数据进行比较,两组数据完全一致;</font></strong></p>
<h3 id="五-使用线性回归-求解斜率和截距"><a href="#五-使用线性回归-求解斜率和截距" class="headerlink" title="(五): 使用线性回归,求解斜率和截距"></a>(五): 使用线性回归,求解斜率和截距</h3><ul>
<li>根据最小二乘法: $\min_{w}||Xw-y||_2^2$ 推到得出公式: $w = (X^TX)^{-1}X^Ty$</li>
</ul>
<p>以上公式只能求出w,我们可以先求出w再计算出b;</p>
<ul>
<li><p>但此处我们有更简单的方法:</p>
<p>根据线性回归方程 $ f(x) = w_1x_1+w_2x_2+b$ 我们可以将方程中的b看成是$w_3x_3^0$, </p>
</li>
</ul>
<p>所以可得: $f(x) = w_1x_1^1+w_2x_2^1+w_3x_3^0$</p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">X, y = datasets.load_boston(<span class="literal">True</span>)</span><br><span class="line">linear = LinearRegression()</span><br><span class="line">linear.fit(X,y)</span><br><span class="line">w_ = linear.coef_</span><br><span class="line">b_ = linear.intercept_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向X中插入一列全是1的数据(任何数的0次方都是1)</span></span><br><span class="line">X = np.concatenate([X, np.ones(shape = (<span class="number">506</span>, <span class="number">1</span>))], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据最小二乘法的推导公式:w和b的值为(最后一个值是b)</span></span><br><span class="line">w = ((np.linalg.inv(X.T.dot(X))).dot(X.T)).dot(y)</span><br><span class="line"><span class="comment"># 以上w的写法过于装逼,所以分解为:</span></span><br><span class="line"><span class="comment"># A = X.T.dot(X)   求X和转置后的X的内积(公式中的XTX)</span></span><br><span class="line"><span class="comment"># B = np.linalg.inv(A)   求A的逆矩阵(公式中的-1次方)</span></span><br><span class="line"><span class="comment"># C = B.dot(X.T)   求以上矩阵和X的转置矩阵的内积(公式中的XT)</span></span><br><span class="line"><span class="comment"># w = C.dot(y)   与y求内积,得出w和b</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">运行结果:</span></span><br><span class="line"><span class="string">array([-1.08011358e-01,  4.64204584e-02,  2.05586264e-02,  2.68673382e+00,</span></span><br><span class="line"><span class="string">       -1.77666112e+01,  3.80986521e+00,  6.92224640e-04, -1.47556685e+00,</span></span><br><span class="line"><span class="string">        3.06049479e-01, -1.23345939e-02, -9.52747232e-01,  9.31168327e-03,</span></span><br><span class="line"><span class="string">       -5.24758378e-01,  3.64594884e+01])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">print(b_)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">运行结果:</span></span><br><span class="line"><span class="string">36.45948838509001</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<h2 id="扩展一-最小二乘法和向量范数"><a href="#扩展一-最小二乘法和向量范数" class="headerlink" title="扩展一: 最小二乘法和向量范数"></a>扩展一: 最小二乘法和向量范数</h2><p>$\min_{w}||Xw-y||_2^2$</p>
<ul>
<li>右上角的2是平方</li>
<li>右下角的2是向量2范数</li>
<li>竖线内的表达式是向量</li>
</ul>
<p><strong>根据最小二乘法的公式, 推导得出</strong></p>
<p>$$w = (X^TX)^{-1}X^Ty$$</p>
<p><strong>向量的1-范数(表示各个元素的绝对值的和)</strong></p>
<p>$$||X||_1 = \sum\limits_{i=1}^n |x_i|$$</p>
<p><strong>向量的2-范数(表示每个元素的平方和再开平方)</strong></p>
<p>$$||X||_2 = \sqrt{\sum\limits_{i=1}^n x_i^2}$$</p>
<p><strong>向量的无穷范数(所有向量元素绝对值中的最大值)</strong></p>
<p>$$||X||_{\infty} = \max\limits_{1 \geq i \leq n}|X_i|$$</p>
<h2 id="扩展二-导数-偏导数"><a href="#扩展二-导数-偏导数" class="headerlink" title="扩展二: 导数, 偏导数"></a>扩展二: 导数, 偏导数</h2><p><strong>导数:</strong><br>对函数$f(x) = x^2+3x+8$ 求导得: $f(x)’ = 2x+3$</p>
<p>求导规则: </p>
<ul>
<li>参数求导为0</li>
<li>参数乘变量求导为常数</li>
<li>变量的次方求导: $x^y$求导为$yx^{y-1}$</li>
<li>复合函数求导:<ul>
<li>$(x^2-x)^2$求导: 先将括号看成一个整体求导, 结果再乘以括号内的求导结果</li>
<li>$2(x^2-x)(2x-1)$</li>
</ul>
</li>
</ul>
<p><strong>偏导数:</strong><br>有多个变量得函数求导:</p>
<p>对函数: $f(x, y) = x^2+xy+y^2$ 求导:</p>
<p>求导规则: 多变量函数只能针对某一个变量求导,此时将其他变量看成常数</p>
<p>将x看成常数a: $f_a(y) = a^2+ay+y^2$</p>
<p>求导得:$f_a’(y) = a+2y$</p>
<p>故求导得: $\frac{\partial f}{\partial y}(x,y)=x+2y$</p>
<p><strong>实现线性回归的两种方式:</strong></p>
<ol>
<li><p>正规方程</p>
</li>
<li><p>梯度下降</p>
</li>
</ol>
<br>



<h1 id="二-正规方程"><a href="#二-正规方程" class="headerlink" title="二: 正规方程"></a>二: 正规方程</h1><h2 id="一-损失函数"><a href="#一-损失函数" class="headerlink" title="(一): 损失函数"></a>(一): 损失函数</h2><p><strong>最小二乘法:</strong></p>
<p>$$\min\limits_{w}||Xw-y||_2^2$$</p>
<p><strong>当X和y都是常数时,按照向量2范数将上面的最小二乘法解开:</strong></p>
<p>$$f(w)=(Xw-y)^2$$</p>
<p><strong>将X,y替换成常数a,b</strong></p>
<p>$$f(w)=(aw-b)^2$$</p>
<p>$$f(w)=a^2w^2 - 2abw + b^2$$</p>
<p>​        由于最小二乘法方程的函数值都是大雨或等于0的,所以此时得到一个开口向</p>
<p>上的抛物线(一元二次方程)</p>
<p>​        此时的$f(w)$就是损失函数,在此时求该函数的导数(抛物线函数顶点的导数为0)</p>
<p>就能得到该函数的最小值,也就是最小损失</p>
<p>$$f’(w)=2a^2w-2ab=0$$</p>
<p><strong>此时即可算出最小的$w$,即最小损失</strong></p>
<h2 id="二-矩阵常用求导公式"><a href="#二-矩阵常用求导公式" class="headerlink" title="(二): 矩阵常用求导公式"></a>(二): 矩阵常用求导公式</h2><p><strong>X的转置矩阵对X矩阵求导, 求解出来是单位矩阵</strong></p>
<ul>
<li><p>$$\frac{dX^T}{dX} = I$$ </p>
</li>
<li><p>$$\frac{dX}{dX^T} = I$$</p>
</li>
</ul>
<p><strong>X的转置矩阵和一个常数矩阵相乘再对X矩阵求导, 求解出来就是改常数矩阵</strong></p>
<ul>
<li>$$\frac{dX^TA}{dX} = A$$ </li>
<li>$$\frac{dAX}{dX} = A^T$$</li>
<li>$$\frac{dXA}{dX} = A^T$$</li>
<li>$$\frac{dAX}{dX^T} = A$$</li>
</ul>
<h2 id="三-正规方程矩阵推导过程"><a href="#三-正规方程矩阵推导过程" class="headerlink" title="(三): 正规方程矩阵推导过程"></a>(三): 正规方程矩阵推导过程</h2><p><font color=red>此时X,w,y都是矩阵</font></p>
<h3 id="1-公式化简"><a href="#1-公式化简" class="headerlink" title="1: 公式化简"></a>1: 公式化简</h3><p><strong>1: 最小二乘法:</strong></p>
<p>$$f(w) = ||Xw-y||_2^2$$</p>
<p><strong>2: 向量2范数:</strong></p>
<p>$$||X||<em>2 = \sqrt{\sum\limits</em>{i = 1}^nx_i^2}$$</p>
<p><strong>3: 将向量2范数的公式带入到最小二乘法中得:</strong></p>
<p>$$f(w)=(\sqrt{(Xw-y)^2})^2$$</p>
<p><strong>4. 化简:</strong></p>
<p>$$f(w)=(Xw-y)^2$$</p>
<p><font color=red>由于X, w, y都是矩阵, 运算后还是矩阵; 矩阵得乘法是一个矩阵得行和另一个矩阵得列相乘; 所以矩阵的平方就是该矩阵乘以他本身的转置矩阵</font></p>
<p><strong>5. 所以:</strong></p>
<p>$$f(w)=(Xw-y)^T(Xw-y)$$</p>
<p><strong>6. 展开:</strong></p>
<p><font color=red>注意: 整体转置变成每个元素都转置时,若是有乘法, 则相乘的两个矩阵要交换位置; 如下所示!!!</font></p>
<p>$$f(w)=(w^TX^T-y^T)(Xw-y)$$</p>
<p>$$f(w)=w^TX^TXw-w^TX^Ty-y^TXw+y^Ty$$</p>
<p><font color=red>注意: 若想交换两个相乘的矩阵再算式中的位置,则交换之后双方都需要转置一次; 如下所示!!!</font></p>
<p>$$f(w)=w^TX^TXw-(X^Ty)^T(w^T)^T-y^TXw+y^Ty$$</p>
<p>$$f(w)=w^TX^TXw-y^TXw-y^TXw+y^Ty$$</p>
<p>$$f(w) = w^TX^TXw - 2y^TXw + y^Ty $$</p>
<h3 id="2-求导"><a href="#2-求导" class="headerlink" title="2: 求导"></a>2: 求导</h3><p>$$f(w) = w^TX^TXw - 2y^TXw + y^Ty $$</p>
<p><strong>这里 $y^Ty$ 是常数求导后为0</strong></p>
<p><strong>$2y^TXw$ 求导:</strong></p>
<p>$$\frac{d(2y^TX)w}{dw}=(2y^TX)^T=2X^T(y^T)^T=2X^Ty$$</p>
<p><strong>$w^TX^TXw$求导:</strong></p>
<p>$$\frac{dw^TX^TXw}{dw}=\frac{d(w^TX^TX)w}{dw}+\frac{dw^T(X^TXw)}{dw}=(w^TX^TX)^T+X^TXw=X^T(X^T)^T(w^T)^T+X^TXw=2X^TXw$$</p>
<p><strong>所以:</strong></p>
<p>$$f’(w) =  2X^TXw - 2X^Ty$$</p>
<p><strong>令$f’(w)=0$,则:</strong></p>
<p>$$2X^TXw - 2X^Ty = 0$$</p>
<p>$$X^TXw=X^Ty$$</p>
<p><strong>矩阵运算没有除法,可以用逆矩阵实现除法的效果</strong></p>
<p><strong>等式两边同时乘以$X^TX$的逆矩阵$(X^TX)^{-1}$</strong></p>
<p>$$(X^TX)^{-1}(X^TX)w=(X^TX)^{-1}X^Ty$$</p>
<p>$$Iw=(X^TX)^{-1}X^Ty$$  <strong>I是单位矩阵</strong></p>
<p><strong>得到正规方程:</strong></p>
<p>$$w=(X^TX)^{-1}X^Ty$$</p>
<h2 id="四-数据挖掘实例-预测2020年淘宝双十一交易额"><a href="#四-数据挖掘实例-预测2020年淘宝双十一交易额" class="headerlink" title="(四): 数据挖掘实例(预测2020年淘宝双十一交易额)"></a>(四): 数据挖掘实例(预测2020年淘宝双十一交易额)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">X = np.arange(<span class="number">2009</span>, <span class="number">2020</span>)   <span class="comment"># 年份</span></span><br><span class="line">X = X <span class="number">-2008</span>   <span class="comment"># 年份数值太大,差别不明显</span></span><br><span class="line">y = np.array([<span class="number">0.5</span>, <span class="number">9.36</span>, <span class="number">52</span>, <span class="number">191</span>, <span class="number">350</span>, <span class="number">571</span>, <span class="number">912</span>, <span class="number">1207</span>, <span class="number">1682</span>, <span class="number">2135</span>, <span class="number">2684</span>])   <span class="comment"># 09年到19年的交易额</span></span><br></pre></td></tr></table></figure>
<p><strong>假设X和y之间是一元三次的关系(按照前几年的数据走势提出的假设)</strong></p>
<p>$$f(x)=w_1x+w_2x^2+w_3x^3+b$$</p>
<p>$$f(x)=w_0x^0+w_1x^1+w_2x^2+w_3x^3$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_oo = np.concatenate([a,a])   # 横着级联</span></span><br><span class="line">X_train = np.c_[X**<span class="number">0</span>, X**<span class="number">1</span>, X**<span class="number">2</span>, X**<span class="number">3</span>]   <span class="comment"># 竖着级联</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[   1,    1,    1,    1],</span></span><br><span class="line"><span class="string">       [   1,    2,    4,    8],</span></span><br><span class="line"><span class="string">       [   1,    3,    9,   27],</span></span><br><span class="line"><span class="string">       [   1,    4,   16,   64],</span></span><br><span class="line"><span class="string">       [   1,    5,   25,  125],</span></span><br><span class="line"><span class="string">       [   1,    6,   36,  216],</span></span><br><span class="line"><span class="string">       [   1,    7,   49,  343],</span></span><br><span class="line"><span class="string">       [   1,    8,   64,  512],</span></span><br><span class="line"><span class="string">       [   1,    9,   81,  729],</span></span><br><span class="line"><span class="string">       [   1,   10,  100, 1000],</span></span><br><span class="line"><span class="string">       [   1,   11,  121, 1331]], dtype=int32)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">linear = LinearRegression(fit_intercept=<span class="literal">False</span>)   <span class="comment"># 声明算法; fit_intercept=False将截距设置为0, w0就是截距</span></span><br><span class="line">linear.fit(X_train, y)   <span class="comment"># 训练</span></span><br><span class="line">w_ = linear.coef_</span><br><span class="line">print(linear.coef_.round(<span class="number">2</span>))   <span class="comment"># 获取系数</span></span><br><span class="line">print(linear.intercept_)   <span class="comment"># 获取截距</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[ 58.77 -84.06  27.95   0.13]</span></span><br><span class="line"><span class="string">0.0</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p><strong>可以得到方程:</strong></p>
<p>$$f(x)=58.77-84.06x+27.95x^2+0.13x^3$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X_test = np.linspace(<span class="number">0</span>,<span class="number">12</span>,<span class="number">126</span>)   <span class="comment"># 线性分割(将0,12之间分成126分)等差数列包含1和12</span></span><br><span class="line">X_test = np.c_[X_test**<span class="number">0</span>, X_test**<span class="number">1</span>, X_test**<span class="number">2</span>, X_test**<span class="number">3</span>]   <span class="comment"># 和训练数据保持一致</span></span><br><span class="line">y_ = linear.predict(X_test)   <span class="comment"># 使用模型预测</span></span><br><span class="line">plt.plot(np.linspace(<span class="number">0</span>,<span class="number">12</span>,<span class="number">126</span>), y_, color=<span class="string">'g'</span>)   <span class="comment"># 绘制预测方程曲线</span></span><br><span class="line">plt.scatter(np.arange(<span class="number">1</span>,<span class="number">12</span>), y, color=<span class="string">'red'</span>)   <span class="comment"># 绘制每年的真实销量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数</span></span><br><span class="line">fun = <span class="keyword">lambda</span> x : w_[<span class="number">0</span>] + w_[<span class="number">1</span>]*x + w_[<span class="number">2</span>]*x**<span class="number">2</span> + w_[<span class="number">-1</span>]*x**<span class="number">3</span></span><br><span class="line"></span><br><span class="line">fun(<span class="number">12</span>)</span><br><span class="line"><span class="string">'''3294.2775757576132'''</span></span><br></pre></td></tr></table></figure>




<br>




<h1 id="三-梯度下降"><a href="#三-梯度下降" class="headerlink" title="三: 梯度下降"></a>三: 梯度下降</h1><p>​        梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下</p>
<p>来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确</p>
<p>定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体</p>
<p>来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，</p>
<p>如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复</p>
<p>采用同一个方法，最后就能成功的抵达山谷。</p>
<h2 id="一-梯度下降"><a href="#一-梯度下降" class="headerlink" title="(一): 梯度下降"></a>(一): 梯度下降</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f = <span class="keyword">lambda</span> w : (w <span class="number">-3.5</span>)**<span class="number">2</span> <span class="number">-4.5</span>*w +<span class="number">10</span></span><br><span class="line">d = <span class="keyword">lambda</span> w : <span class="number">2</span>*(w<span class="number">-3.5</span>)<span class="number">-4.5</span>   <span class="comment"># 梯度 == 导数</span></span><br><span class="line">step = <span class="number">0.1</span>   <span class="comment"># 梯度下降的步幅,比率,学习率(默认是1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求当w(x值)为多少时,得到的函数值最小</span></span><br><span class="line"></span><br><span class="line">w = np.random.randint(<span class="number">0</span>,<span class="number">11</span>,size=<span class="number">1</span>)[<span class="number">0</span>]   <span class="comment"># 随机生成一个初始值数</span></span><br><span class="line">last_w = w + <span class="number">0.1</span>   <span class="comment"># 梯度下降,每走一步,目标值,都会更新</span></span><br><span class="line">precision = <span class="number">1e-4</span>   <span class="comment"># 精确率(误差率,越小越精确,并不是越小越好)</span></span><br><span class="line">print(<span class="string">'==============更新前的w:'</span>, w)</span><br><span class="line">w_ = [w]</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">if</span> np.abs(w - last_w) &lt; precision:    <span class="comment"># 退出条件</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    last_w = w    <span class="comment"># 更新</span></span><br><span class="line">    w -= step*d(w)   </span><br><span class="line">    <span class="comment"># 随机初始值在目标值的左边时, d(w)为负, w = w-step*d(w) 会使得w慢慢变大趋近目标值</span></span><br><span class="line">    <span class="comment"># 随机初始值在目标值的右边时, d(w)为正, w = w-step*d(w) 会使得w慢慢变小趋近目标值</span></span><br><span class="line">    w_.append(w)</span><br><span class="line">    print(<span class="string">'==============更新后的w:'</span>, w)</span><br></pre></td></tr></table></figure>




<h2 id="二-梯度下降实现线性回归"><a href="#二-梯度下降实现线性回归" class="headerlink" title="(二): 梯度下降实现线性回归"></a>(二): 梯度下降实现线性回归</h2><h3 id="1-构造数据"><a href="#1-构造数据" class="headerlink" title="1. 构造数据"></a>1. 构造数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X = np.linspace(<span class="number">-2</span>, <span class="number">12</span>, <span class="number">40</span>).reshape(<span class="number">40</span>,<span class="number">1</span>)   </span><br><span class="line"><span class="comment"># reshape(40,1)改变形状; 40这个位置可以写成-1, 此时-1代表40; 当后面的1变为2时,此时-1代表20; 也就是说当后面的数值变化时,系统自动用前面规定的总数计算出相应的结果,而这些结果都可以用-1代替; </span></span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">9</span>,size = <span class="number">1</span>)   <span class="comment"># 系数</span></span><br><span class="line">b = np.random.randint(<span class="number">-5</span>,<span class="number">5</span>,size = <span class="number">1</span>)   <span class="comment"># 截距</span></span><br><span class="line">y = w*X + b + np.random.randn(<span class="number">40</span>,<span class="number">1</span>) * <span class="number">2</span>   <span class="comment"># 增加噪声</span></span><br><span class="line">plt.scatter(X, y)   <span class="comment"># 绘点</span></span><br></pre></td></tr></table></figure>



<h3 id="2-使用sklearn中的回归-梯度下降方法计算斜率和截距"><a href="#2-使用sklearn中的回归-梯度下降方法计算斜率和截距" class="headerlink" title="2. 使用sklearn中的回归 - 梯度下降方法计算斜率和截距"></a>2. 使用sklearn中的回归 - 梯度下降方法计算斜率和截距</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">linear = LinearRegression()   <span class="comment"># 定义算法</span></span><br><span class="line">linear.fit(X, y)  <span class="comment"># 训练</span></span><br><span class="line">X_test = np.linspace(<span class="number">-2</span>,<span class="number">12</span>,<span class="number">256</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)   <span class="comment"># 预测数据</span></span><br><span class="line">y_ = linear.predict(X_test)   <span class="comment"># 预测结果</span></span><br><span class="line">print(<span class="string">'真实斜率和截距'</span>, w, b)</span><br><span class="line">print(<span class="string">'算法计算的斜率和截距'</span>, linear.coef_, linear.intercept_)  <span class="comment"># 截距和斜率</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">真实斜率和截距 [5] [3]</span></span><br><span class="line"><span class="string">算法计算的斜率和截距 [[4.91521288]] [3.06426015]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">plt.plot(X_test, y_, color=<span class="string">'g'</span>)   <span class="comment"># 绘制线性回归方程</span></span><br><span class="line">plt.scatter(X, y, color=<span class="string">'r'</span>)   <span class="comment"># 绘制实际的点</span></span><br></pre></td></tr></table></figure>



<h3 id="3-自定义梯度下降的类实现sklearn中的回归-梯度功能"><a href="#3-自定义梯度下降的类实现sklearn中的回归-梯度功能" class="headerlink" title="3.自定义梯度下降的类实现sklearn中的回归 - 梯度功能"></a>3.自定义梯度下降的类实现sklearn中的回归 - 梯度功能</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span>   <span class="comment"># 初始化, 随机给定截距和斜率</span></span><br><span class="line">        self.w = np.random.randn(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        self.b = np.random.randn(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w*x + self.b   <span class="comment"># 一元一次线性方程; 模型</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self,x,y)</span>:</span><span class="comment">#损失，最小二乘法</span></span><br><span class="line">        cost = (self.model(x) - y)**<span class="number">2</span> <span class="comment"># 损失函数越小越好</span></span><br><span class="line">        <span class="comment"># 求解梯度，两个未知数，所以，偏导</span></span><br><span class="line">        d_w = <span class="number">2</span>*(self.model(x) - y)*x <span class="comment"># 斜率w的偏导</span></span><br><span class="line">        d_b = <span class="number">2</span>*(self.model(x) - y)*<span class="number">1</span> <span class="comment"># 截距b的偏导</span></span><br><span class="line">        <span class="keyword">return</span> cost,d_w,d_b</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(self, step, d_w, d_b)</span>:</span>   <span class="comment"># 梯度下降</span></span><br><span class="line">        self.w -= step*d_w  <span class="comment"># 更新斜率</span></span><br><span class="line">        self.b -= step*d_b  <span class="comment"># 更新截距</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span>   <span class="comment"># 训练模型, 将数据给模型,寻找规律</span></span><br><span class="line">        precision = <span class="number">1e-4</span>   <span class="comment"># 精确度</span></span><br><span class="line">        last_w = self.w + <span class="number">0.01</span></span><br><span class="line">        last_b = self.b + <span class="number">0.01</span></span><br><span class="line">        </span><br><span class="line">        print(<span class="string">'------------------------初始的截距和斜率:'</span>, self.w, self.b)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">if</span> (np.abs(self.w-last_w) &lt; precision) &amp; (np.abs(self.b-last_b) &lt; precision):</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            last_w = self.w   <span class="comment"># 更新之前,先保留记录</span></span><br><span class="line">            last_b = self.b</span><br><span class="line">            </span><br><span class="line">            cost_ = <span class="number">0</span></span><br><span class="line">            dw_ = <span class="number">0</span></span><br><span class="line">            db_ = <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>):   <span class="comment"># 计算40个，返回40个偏导数，求平均值</span></span><br><span class="line">                cost,dw,db = self.loss(X[i,<span class="number">0</span>],y[i,<span class="number">0</span>])</span><br><span class="line">                cost_ += cost/<span class="number">40</span></span><br><span class="line">                dw_ += dw/<span class="number">40</span></span><br><span class="line">                db_ += db/<span class="number">40</span></span><br><span class="line">                </span><br><span class="line">            self.gradient_descent(<span class="number">0.01</span>, dw_, db_)</span><br><span class="line">            print(<span class="string">'------------------------更新后的截距和斜率:'</span>, self.w, self.b)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.model(X)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">model = LinearModel()   <span class="comment"># 定义算法</span></span><br><span class="line">w, b = model.fit(X, y)   <span class="comment"># 获取斜率和截距</span></span><br><span class="line">y_ = model.predict(X_test)   <span class="comment"># 用测试数据获取目标值</span></span><br><span class="line">plt.plot(X_test, y_, color=<span class="string">'g'</span>)   <span class="comment"># 绘制线性方程</span></span><br><span class="line">plt.scatter(X, y, color=<span class="string">'r'</span>)   <span class="comment"># 绘制目标点</span></span><br></pre></td></tr></table></figure>


<h2 id="三-梯度下降-矩阵"><a href="#三-梯度下降-矩阵" class="headerlink" title="(三): 梯度下降(矩阵)"></a>(三): 梯度下降(矩阵)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造数据</span></span><br><span class="line">X = np.linspace(<span class="number">-2</span>, <span class="number">12</span>, <span class="number">40</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">w = np.random.randint(<span class="number">2</span>, <span class="number">12</span>, size=<span class="number">1</span>)</span><br><span class="line">b = np.random.randint(<span class="number">-10</span>, <span class="number">10</span>, size=<span class="number">1</span>)</span><br><span class="line">y = X*w + b + np.random.randn(<span class="number">40</span>, <span class="number">1</span>)*<span class="number">2.5</span></span><br><span class="line">plt.scatter(X, y, color=<span class="string">'r'</span>)</span><br><span class="line">X = np.concatenate([X, np.ones(shape=(<span class="number">40</span>, <span class="number">1</span>))], axis=<span class="number">1</span>)  <span class="comment"># 将截距b看作一个斜率w, 在下面定义的类中要求输入两组X值, 所以此处将X的数据结构变为(40,2)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义梯度下降类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>) <span class="comment"># theta中既有斜率，又有截距</span></span><br><span class="line">    last_theta = theta + <span class="number">0.1</span></span><br><span class="line">    precision = <span class="number">1e-4</span></span><br><span class="line">    epsilon = <span class="number">0.01</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 当斜率和截距误差小于万分之一时，退出</span></span><br><span class="line">        <span class="keyword">if</span> (np.abs(theta - last_theta) &lt; precision).all():</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 更新</span></span><br><span class="line">        last_theta = theta.copy()</span><br><span class="line">        theta = theta - epsilon*<span class="number">2</span>/<span class="number">40</span>*X.T.dot(X.dot(theta) - y)</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用</span></span><br><span class="line">w_,b_ = gradient_descent(X,y)</span><br><span class="line">j = <span class="keyword">lambda</span> x : w_*x + b_</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],y,color = <span class="string">'red'</span>)</span><br><span class="line">x_test = np.linspace(<span class="number">-2</span>,<span class="number">12</span>,<span class="number">1024</span>) </span><br><span class="line">y_ = j(x_test)</span><br><span class="line">plt.plot(x_test,y_,color = <span class="string">'green'</span>)</span><br></pre></td></tr></table></figure>


<h2 id="四-随机梯度下降-矩阵-从所有样本中随机获取指定数量的样本"><a href="#四-随机梯度下降-矩阵-从所有样本中随机获取指定数量的样本" class="headerlink" title="(四): 随机梯度下降(矩阵) - 从所有样本中随机获取指定数量的样本"></a>(四): 随机梯度下降(矩阵) - 从所有样本中随机获取指定数量的样本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造数据</span></span><br><span class="line">X = np.linspace(<span class="number">-2</span>,<span class="number">12</span>,<span class="number">40</span>).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">w = np.random.randint(<span class="number">2</span>,<span class="number">12</span>,size = <span class="number">1</span>)</span><br><span class="line">b = np.random.randint(<span class="number">-10</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X*w + b + np.random.randn(<span class="number">40</span>,<span class="number">1</span>)*<span class="number">2.5</span></span><br><span class="line"><span class="comment"># 将y.reshape(-1)一维的</span></span><br><span class="line">y = y.reshape(<span class="number">-1</span>)</span><br><span class="line">plt.scatter(X,y,color = <span class="string">'red'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义梯度下降类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    m = <span class="number">10</span>   <span class="comment"># 随机样本数量</span></span><br><span class="line">    theta = np.random.randn(<span class="number">2</span>) <span class="comment"># theta中既有斜率，又有截距</span></span><br><span class="line">    last_theta = theta + <span class="number">0.1</span> <span class="comment">#记录theta更新后，和上一步的误差</span></span><br><span class="line">    precision = <span class="number">1e-4</span> <span class="comment">#精确度</span></span><br><span class="line">    epsilon = <span class="number">0.01</span> <span class="comment">#步幅</span></span><br><span class="line">    count= <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 当斜率和截距误差小于万分之一时，退出</span></span><br><span class="line">        <span class="keyword">if</span> (np.abs(theta - last_theta) &lt; precision).all():</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> count &gt; <span class="number">3000</span>:<span class="comment">#死循环执行了3000次</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 更新</span></span><br><span class="line">        last_theta = theta.copy()</span><br><span class="line">        <span class="comment"># 随机梯度下降，梯度是矩阵计算返回的</span></span><br><span class="line">        index = np.random.choice(np.arange(<span class="number">40</span>),size = m)<span class="comment"># index索引，根据随机索引从原数据中取数据</span></span><br><span class="line">        grad = <span class="number">2</span>/m*X[index].T.dot(X[index].dot(theta) - y[index])</span><br><span class="line">        theta -= epsilon*grad</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line">w_,b_ = gradient_descent(X_train,y)</span><br><span class="line">j = <span class="keyword">lambda</span> x : w_*x + b_</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],y,color = <span class="string">'red'</span>)</span><br><span class="line">x_test = np.linspace(<span class="number">-2</span>,<span class="number">12</span>,<span class="number">1024</span>) </span><br><span class="line">y_ = j(x_test)</span><br><span class="line">plt.plot(x_test,y_,color = <span class="string">'green'</span>)</span><br></pre></td></tr></table></figure>

<p><font color=red>若随机样本的数量为1, 则是完全随机梯度下降</font></p>
<br>



<h1 id="四-岭回归"><a href="#四-岭回归" class="headerlink" title="四: 岭回归"></a>四: 岭回归</h1><h2 id="一-岭回归和线性回归的模型对比"><a href="#一-岭回归和线性回归的模型对比" class="headerlink" title="(一): 岭回归和线性回归的模型对比"></a>(一): 岭回归和线性回归的模型对比</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">X, y = datasets.load_boston(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性回归</span></span><br><span class="line">linear = LinearRegression()</span><br><span class="line">linear.fit(X, y)</span><br><span class="line">w_ = linear.coef_</span><br><span class="line">b_ = linear.intercept_</span><br><span class="line">print(<span class="string">'普通线性回归斜率:\n'</span>, w_)</span><br><span class="line">print(<span class="string">'普通线性回归截距:'</span>, b_)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">普通线性回归斜率:</span></span><br><span class="line"><span class="string"> [-1.08011358e-01  4.64204584e-02  2.05586264e-02  2.68673382e+00</span></span><br><span class="line"><span class="string"> -1.77666112e+01  3.80986521e+00  6.92224640e-04 -1.47556685e+00</span></span><br><span class="line"><span class="string">  3.06049479e-01 -1.23345939e-02 -9.52747232e-01  9.31168327e-03</span></span><br><span class="line"><span class="string"> -5.24758378e-01]</span></span><br><span class="line"><span class="string">普通线性回归截距: 36.45948838509001</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 岭回归</span></span><br><span class="line">ridge = Ridge(alpha=<span class="number">10</span>)</span><br><span class="line">ridge.fit(X, y)</span><br><span class="line">w_ = ridge.coef_</span><br><span class="line">b_ = ridge.intercept_</span><br><span class="line">print(<span class="string">'岭回归斜率:\n'</span>, w_)</span><br><span class="line">print(<span class="string">'岭回归截距:'</span>, b_)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">岭回归斜率:</span></span><br><span class="line"><span class="string"> [-0.10143535  0.0495791  -0.0429624   1.95202082 -2.37161896  3.70227207</span></span><br><span class="line"><span class="string"> -0.01070735 -1.24880821  0.2795956  -0.01399313 -0.79794498  0.01003684</span></span><br><span class="line"><span class="string"> -0.55936642]</span></span><br><span class="line"><span class="string">岭回归截距: 27.467884964141252</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>


        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>文章作者:</span>
                        <span>Anjhon</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>文章地址:</span>
                        <span><a href="https://anjhon1994.github.io/2020/03/06/sklearn%20-%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92(%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D)/">sklearn - 线性回归(正规方程与梯度下降)</a></span>
                    </p>
                
                
                
                     <p class="copyright-item">
                         <span>灵魂拷问:</span>
                         <span>Do you believe in <strong>DESTINY<strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">✓ 机器学习</a>
                    
                        <a href="/tags/%E5%9F%BA%E7%A1%80/">✓ 基础</a>
                    
                        <a href="/tags/sklearn/">✓ sklearn</a>
                    
                        <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">✓ 线性回归</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/03/10/sklearn%20-%20%E5%B2%AD%E5%9B%9E%E5%BD%92(Ridge)%E5%92%8C%E5%A5%97%E7%B4%A2%E5%9B%9E%E5%BD%92(Lasso)/">sklearn - 岭回归(Ridge)和套索回归(Lasso)</a>
            
            
            <a class="next" rel="next" href="/2020/02/13/KNN%E7%AE%97%E6%B3%95/">KNN近邻算法</a>
            
        </section>


    </article>
</div>

			
        </div>
		
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© 
		Anjhon | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a>
		
		
		
		<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


		
		
		</span>
    </div>
</footer>

    </div>
	
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/ Relative)","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"left","width":180,"height":350},"mobile":{"show":false},"react":{"opacityDefault":0.7}});</script></body>
</html>
