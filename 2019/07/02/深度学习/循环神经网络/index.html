<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">

<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Anjhon">


    <meta name="subtitle" content="小安">


    <meta name="description" content="佛系分享">



<title>循环神经网络 | Anjhon</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    





    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    





    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        
		src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 4.1.1"></head>
<body>

	
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Anjhon</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">博文</a>
                
                    <a class="menu-item" href="/category">分类</a>
                
                    <a class="menu-item" href="/tag">标签</a>
                
                    <a class="menu-item" href="/about">关于</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Anjhon</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">博文</a>
                
                    <a class="menu-item" href="/category">分类</a>
                
                    <a class="menu-item" href="/tag">标签</a>
                
                    <a class="menu-item" href="/about">关于</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">展开全部</a>
        <a onclick="go_top()">回到顶部</a>
        <a onclick="go_bottom()">下到底部</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "折叠起来"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "展开全部"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">循环神经网络</h1>
            
                <div class="post-meta">
                    <br>
                    
                        作者: <a itemprop="author" rel="author" href="/">&nbsp;&nbsp;Anjhon</a>
                    

                    
                        <p class="post-time">
                        发表: <a href="#">&nbsp;&nbsp;2019-07-02</a>
                        </p>
                    
                    
                        <p class="post-category">
                    分类:
                            
                                <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">&nbsp;&nbsp;深度学习</a>
                            
                            </p>
                    
                </div>
            
        </header>

        <div class="post-content">
            <blockquote>
<p>本文整理自:</p>
<p>原理推导:</p>
<p><a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">零基础入门深度学习(5) - 循环神经网络</a></p>
<p><a href="https://zybuluo.com/hanbingtao/note/581764" target="_blank" rel="noopener">零基础入门深度学习(6) - 长短时记忆网络(LSTM)</a></p>
<p>原理讲解:</p>
<p><a href="https://zhuanlan.zhihu.com/p/123211148" target="_blank" rel="noopener">史上最详细循环神经网络讲解（RNN/LSTM/GRU）</a></p>
</blockquote>
<h1 id="一-循环神经网络RNN"><a href="#一-循环神经网络RNN" class="headerlink" title="一: 循环神经网络RNN"></a>一: 循环神经网络RNN</h1><p>某些任务需要能够更好的处理<strong>序列</strong>的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个<strong>序列</strong>；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个<strong>序列</strong>。这时，就需要用到深度学习领域中另一类非常重要神经网络：<strong>循环神经网络(Recurrent Neural Network)</strong>。</p>
<p>RNN的特点，<strong>RNN对具有序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息，</strong>利用了RNN的这种能力，使深度学习模型在解决语音识别、语言模型、机器翻译以及时序分析等NLP领域的问题时有所突破。</p>
<h2 id="1-基本循环神经网络"><a href="#1-基本循环神经网络" class="headerlink" title="1: 基本循环神经网络"></a>1: 基本循环神经网络</h2><p>下图是一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：</p>
<img src="http://ww1.sinaimg.cn/large/e3ee7ad0gy1gn5o8fz9lij203s08vdfx.jpg" alt="基本RNN结构.png" style="zoom:67%;" />



<p><strong>如果将右边的w部分先拿掉, 那剩下的就是最基本的神经网络连接图,</strong> </p>
<ul>
<li><p>X是输入, </p>
</li>
<li><p>U是输入层到隐藏层的参数矩阵(权重矩阵), </p>
</li>
<li><p>S是是隐藏层向量, 是通过X和U的线性组合之后再通过激活函数得到</p>
</li>
<li><p>V是隐藏层到输出层的参数矩阵, </p>
</li>
<li><p>O是输出</p>
</li>
</ul>
<p>那W是什么? <strong>W可以理解为每个时间点之间的参数矩阵(权重矩阵)</strong></p>
<p>将上面的图展开:</p>
<img src="http://ww1.sinaimg.cn/large/e3ee7ad0gy1gn5o8tldymj20m308vaaz.jpg" alt="基本RNN展开结构.png" style="zoom:67%;" />



<p>如上图: 这个网络在t时刻接收到输入$X_t$之后，隐藏层的值是$S_t$，输出值是$O_t$。关键一点是，$S_t$的值不仅仅取决于$X_t$，还取决于$S_{t-1}$。我们可以用下面的公式来表示<strong>循环神经网络</strong>的计算方法：<br>$$<br>o_t = g(Vs_t)\<br>s_t = f(Ux_t+Ws_{t-1})<br>$$<br><strong>值得注意的一点是，在整个训练过程中，每一时刻所用的都是同样的W。</strong></p>
<h2 id="2-双向循环神经网络"><a href="#2-双向循环神经网络" class="headerlink" title="2: 双向循环神经网络"></a>2: 双向循环神经网络</h2><p>在很多时候, 语言处理光看前面的词是不够的，比如下面这句话：</p>
<blockquote>
<p>我的手机坏了，我打算____一部新手机。</p>
</blockquote>
<p>可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的。但如果我们也看到了横线后面的词是『一部新手机』，那么，横线上的词填『买』的概率就大得多了。</p>
<p>这个时候, 我们就需要<strong>双向循环神经网络</strong>了</p>
<img src="http://ww1.sinaimg.cn/large/e3ee7ad0gy1gn5o96z8p4j21080codje.jpg" alt="双向循环神将网络.png" style="zoom:67%;" />



<p>从上图可以看出，<strong>双向卷积神经网络</strong>的隐藏层要保存两个值，一个A参与正向计算，另一个值A’参与反向计算。最终的输出值$y_2$取决于$A_2$和$A’_2$。其计算方法为：<br>$$<br>y_2 = g(VA_2 + V’A’_2)<br>$$</p>
<p>$A_2$和$A’_2$则分别计算：<br>$$<br>A_2 = f(WA_1 + Ux_2)\<br>A’_2 = f(W’A’_3 + U’x_2)<br>$$</p>
<p>可以看出: 正向计算时，隐藏层的值$s_t$与$s_{t-1}$有关；反向计算时，隐藏层的值$s’<em>t$与$s</em>{t+1}$有关；最终的输出取决于正向和反向计算的<strong>加和</strong>。</p>
<p>双向循环神经网络的计算方法：<br>$$<br>o_t = g(Vs_t + V’s’_t)<br>$$</p>
<p>$$<br>s_t = f(Ux_t + Ws_{t-1})<br>$$</p>
<p>$$<br>s’_t = f(U’x_t + W’s’_{t+1})<br>$$</p>
<p>从上面三个公式我们可以看到，正向计算和反向计算<strong>不共享权重</strong>，也就是说U和U’、W和W’、V和V’都是不同的<strong>权重矩阵</strong>。</p>
<p>堆叠两个以上的隐藏层，这样就得到了<strong>深度循环神经网络</strong>。如下图所示：</p>
<img src="http://ww1.sinaimg.cn/large/e3ee7ad0gy1gn5o9ik2sjj20dc0epgn6.jpg" alt="深度循环神经网络.png" style="zoom:67%;" />







<br>



<h1 id="二-长短时记忆网络-LSTM"><a href="#二-长短时记忆网络-LSTM" class="headerlink" title="二: 长短时记忆网络(LSTM)"></a>二: 长短时记忆网络(LSTM)</h1><p>RNN每一时刻的隐藏状态都不仅由该时刻的输入决定，还取决于上一时刻的隐藏层的值，如果一个句子很长，到句子末尾时，它将记不住这个句子的开头的内容详细内容</p>
<p><strong>LSTM是一种改进之后的循环神经网络</strong></p>
<p>RNN是<strong>每个时刻都会把隐藏层的值存下来，到下一时刻的时候再拿出来用，这样就保证了，每一时刻含有上一时刻的信息</strong>，如图，我们把存每一时刻信息的地方叫做Memory Cell，中文就是记忆细胞。</p>
<p><strong>而LSTM则会通过它特有的门控机制选择相应的信息</strong></p>
<img src="http://ww1.sinaimg.cn/large/e3ee7ad0gy1gn5o9sh1b7j20gq0a83z4.jpg" alt="LMST原理.png" style="zoom:67%;" />



<ol>
<li>Input Gate：中文是输入门，在每一时刻从输入层输入的信息会首先经过输入门，输入门的开关会决定这一时刻是否会有信息输入到Memory Cell(每一时刻信息的地方)。</li>
<li>Output Gate：中文是输出门，每一时刻是否有信息从Memory Cell输出取决于这一道门。</li>
<li>Forget Gate：中文是遗忘门，每一时刻Memory Cell里的值都会经历一个是否被遗忘的过程，就是由该门控制的，如果打卡，那么将会把Memory Cell里的值清除，也就是遗忘掉。</li>
</ol>
<p><strong>LSTM和普通RNN正是贵族和乞丐，RNN什么信息它都存下来，因为它没有挑选的能力，而LSTM不一样，它会选择性的存储信息，因为它能力强，它有门控装置，它可以尽情的选择。</strong></p>
<p><strong>信息在传递的顺序，是这样的：</strong></p>
<p>先经过输入门，看是否有信息输入，再判断遗忘门是否选择遗忘Memory Cell里的信息，最后再经过输出门，判断是否将这一时刻的信息进行输出。</p>
<p><strong>LSTM内部结构图</strong></p>
<img src="http://ww1.sinaimg.cn/large/e3ee7ad0gy1gn5oa1yothj20cd0e074h.jpg" alt="LSTM内部结构图.png" style="zoom:67%;" />

<p>LSTM里常用的激活函数有两个，一个是tanh，一个是sigmoid。</p>
<p>其中:<br>$$<br>Z = tanh(W[x_t, h_{t-1}])\\<br>Z_i = sigmoid(W_i[x_t, h_{t-1}])\\<br>Z_f = sigmoid(W_f[x_t, h_{t-1}])\\<br>Z_o = sigmoid(W_o[x_t, h_{t-1}])<br>$$<br><strong>其中: W为各个时刻之间的权重矩阵, $h_{t-1}$是上一时刻的隐藏层信息, 并且每个门的权重向量都不一样</strong></p>
<p>其中 $Z$ 是最为普通的输入，可以从上图中看到， $Z$ 是通过该时刻的输入 $X_t$ 和上一时刻存在memory cell里的隐藏层信息 $h_{t-1}$ 向量拼接，再与权重参数向量点积，得 $W$ 到的值经过激活函数tanh最终会得到一个数值，也就是 $Z$ ，注意只有 $Z$ 的激活函数是tanh，因为 $Z$ 是真正作为输入的，其他三个都是门控装置。</p>
<p>再来看 $Z_i$ ，input gate的缩写i，所以也就是输入门的门控装置， $Z_i$ 同样也是通过该时刻的输入和上 $X_t$ 一时刻隐藏状态，也就是上一时刻存下来的信息 $h_{t-1}$ 向量拼接，在与权重参数向量点积（注 $W$ 意每个门的权重向量都不一样，这里的下标i代表input的意思，也就是输入门）。得到的值经过激活函数sigmoid的最终会得到一个0-1之间的一个数值，用来作为输入门的控制信号。</p>
<p>首先解释一下，经过这个sigmod激活函数后，得到的 $Z_i, Z_f, Z_o$  都是在0到1之间的数值，1表示该门完全打开，0表示该门完全关闭，</p>
<br>



<h1 id="三-梯度消失与梯度爆炸"><a href="#三-梯度消失与梯度爆炸" class="headerlink" title="三: 梯度消失与梯度爆炸"></a>三: 梯度消失与梯度爆炸</h1><p>梯度消失和梯度爆炸其实是一种情况：均是在神经网络中，<strong>当前面隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。</strong></p>
<p>其实梯度爆炸和梯度消失问题都是因为<strong>网络太深</strong>，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。</p>
<p><strong>梯度消失产生的原因：</strong></p>
<p>（1）隐藏层的层数过多;</p>
<p>（2）采用了不合适的激活函数（更容易产生梯度消失，但是也有可能产生梯度爆炸）</p>
<p><strong>梯度爆炸产生的原因：</strong></p>
<p>（1）隐藏层的层数太多;</p>
<p>（2）权重初始化值过大。</p>
<p>1、为什么说隐藏层数过多会造成梯度消失或梯度爆炸？</p>
<p>从深层网络角度来讲，不同的层学习的速度差异很大，表现为网路中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前基层的权值和刚开始随机初始化的值差不多。因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足。</p>
<p>2、激活函数不合适</p>
<p>损失函数对某个权重求偏导, 用求导的链式法则进行分解, 求导的结果与权重W和激活函数的导数有关, $|\sigma’(z)w| \leq 0.25 $, 而sigmoid函数求导后最大最大也只能是0.25,  初始化权重参数W时，通常都小于1, 多个小于1的数连乘之后，那将会越来越小，导致靠近输入层的层的权重的偏导几乎为0，也就是说几乎不更新，这就是梯度消失的根本原因。</p>
<p>再来看看梯度爆炸的原因，也就是说如果$|\sigma’(z)w| \geq 1 $时，连乘下来就会导致梯度过大，导致梯度更新幅度特别大，可能会溢出，导致模型无法收敛。sigmoid的函数是不可能大于1了，上图看的很清楚，那只能是w了，这也就是经常看到别人博客里的一句话，<strong>初始权重过大</strong>，一直不理解为啥。。现在明白了。</p>
<p>3、初始化权重的值过大</p>
<p>也就是w比较大的情况下，根据2式的链式相乘可得(反向传播)，则前面的网络层比后面的网络层梯度变化更快，引起了梯度爆炸的问题。所以，在一般的神经网络中，权重的初始化一般都利用高斯分布随机产生权重值。</p>
<ol start="10">
<li>梯度消失和梯度爆炸的问题是如何产生的？如何解决？</li>
</ol>
<p>答：第一个问题相对简单，由于反向传播过程中，前面网络权重的偏导数的计算是逐渐从后往前累乘的，如果使用 $\sigma, tanh$ 激活函数的话，由于导数小于一，因此累乘会逐渐变小，导致梯度消失，前面的网络层权重更新变慢；如果权重 $w$ 本身比较大，累乘会导致前面网络的参数偏导数变大，产生数值上溢。</p>
<p>因为 sigmoid 导数最大为1/4，故只有当abs(w)&gt;4时才可能出现梯度爆炸，因此最普遍发生的是梯度消失问题。</p>
<p>解决方法通常包括</p>
<ol>
<li>使用ReLU等激活函数，梯度只会为0或者1，每层的网络都可以得到相同的更新速度</li>
<li>采用LSTM</li>
<li>进行梯度裁剪(clip), 如果梯度值大于某个阈值，我们就进行梯度裁剪，限制在一个范围内</li>
<li>使用正则化，这样会限制参数 $w$ 的大小，从而防止梯度爆炸</li>
<li>设计网络层数更少的网络进行模型训练</li>
<li>batch normalization</li>
</ol>
<p><strong>梯度消失与梯度爆炸的解决方案：</strong></p>
<p>（1）预训练加微调</p>
<p>（2）梯度剪切、正则</p>
<p>（3）ReLU、LeakyReLU、ELU等激活函数</p>
<p>（4）BatchNormalization</p>
<p>（5）残差结构</p>
<p>（6）LSTM</p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>文章作者:</span>
                        <span>Anjhon</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>文章地址:</span>
                        <span><a href="https://anjhon1994.github.io/2019/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">循环神经网络</a></span>
                    </p>
                
                
                
                     <p class="copyright-item">
                         <span>灵魂拷问:</span>
                         <span>Do you believe in <strong>DESTINY<strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>文章标签:</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">▼ 深度学习</a>
                    
                        <a href="/tags/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">▼ 递归神经网络</a>
                    
                        <a href="/tags/RNN/">▼ RNN</a>
                    
                        <a href="/tags/LSTM/">▼ LSTM</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">【返回上一层】</a>
                <span>· </span>
                <a href="/">【去往首页】</a>
            </div>
        </section>
        <section class="post-nav">
            	
                <a class="prev" rel="prev" href="/2019/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">上一篇：卷积神经网络</a>
            
            
            <a class="next" rel="next" href="/2019/07/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">下一篇：神经网络</a>
            
			<br>

        </section>


		<br>
		
		
			<span>留下你的痕迹😀</span>
			<section id="comments" class="comments">
			  <style>
				.comments{margin:10px;padding:10px;background:#fff;bordercolor:0,0,0}
				@media screen and (max-width:900px){.comments{margin:auto;padding:20px;background:#fff;bordercolor:0,0,0}}
			  </style>
			  <div id="vcomment" class="comment"></div> 
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script src="https://cdnjs.loli.net/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
<script>
    var notify = 'false' == true ? true : false;
    var verify = 'false' == true ? true : false;
    new Valine({
        av: AV,
        el: '#vcomment',
        notify: notify,
        app_id: "d8Xy9CoRkV8wkjgNsf6IRqfe-gzGzoHsz",
        app_key: "ePiIX7dqVFpzeKkr4T1ixiah",
        placeholder: "🎈🎈🎈 Just say say ...",
        avatar:"robohash",

		
		
		
    });
</script>

			</section>
		
		


    </article>
</div>





			
        </div>
		
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© 
		Anjhon | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a>
		
		
		
		
		</span>
    </div>
</footer>

    </div>
	


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/ Relative)","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"left","width":180,"height":350},"mobile":{"show":false},"react":{"opacityDefault":0.7}});</script></body>
</html>
